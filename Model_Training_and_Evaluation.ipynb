{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: Tongshu Zheng from Duke University\n",
    "#Email: tongshu.zheng@duke.edu; contact me if you have any questions regarding the code\n",
    "#Please reference the code source and publication (i.e., \"Estimating ground-level PM2.5 using micro-satellite \n",
    "#images by a convolutional neural network and random forest approach\") if you use the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "PROJECT_SAVE_DIR = \"Project\"\n",
    "\n",
    "import os\n",
    "if not (os.path.isdir(PROJECT_ROOT_DIR+'/'+PROJECT_SAVE_DIR)):\n",
    "    print('Figure directory didn''t exist, creating now.')\n",
    "    os.mkdir(PROJECT_ROOT_DIR+'/'+PROJECT_SAVE_DIR)\n",
    "else:\n",
    "    print('Figure directory exists.') \n",
    "    \n",
    "def savepdf(fig,name):\n",
    "    fig.savefig(PROJECT_ROOT_DIR+'/'+PROJECT_SAVE_DIR+'/'+name+'.pdf')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "from itertools import compress\n",
    "from scipy import stats\n",
    "import copy\n",
    "import cv2\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from scipy.interpolate import interp1d\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import json\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import time\n",
    "import shlex, subprocess\n",
    "import urllib.request\n",
    "import webbrowser\n",
    "import copy\n",
    "import imageio\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "from keras import applications\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import  image\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, GlobalMaxPooling2D, Conv2D, MaxPooling2D, Activation\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, Callback,LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Load in the Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_X_images_folder = './X_image_full'\n",
    "image_files = [f for f in listdir(my_X_images_folder) if isfile(join(my_X_images_folder, f)) and '.DS_Store' not in f ]\n",
    "image_files_sorted = sorted(image_files, key = lambda x: int(x.split('.')[0].split('_')[2]))\n",
    "\n",
    "image_files_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use keras to preprocess images so that they have the correct forms for the ImageNet CNN models later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_size = 67 #200 m * 200 m model\n",
    "#image_size = 167 #500 m * 500 m model\n",
    "image_size = 224 #670 m * 670 m model\n",
    "#image_size = 33 #100 m * 100 m model\n",
    "X_image = []\n",
    "for image_file in image_files_sorted:\n",
    "    img = image.load_img(my_X_images_folder+'/'+image_file,target_size=(image_size, image_size))\n",
    "    x = image.img_to_array(img) #(224,224,3)\n",
    "    x = np.expand_dims(x, axis=0) #(1,224,224,3)\n",
    "    x = preprocess_input(x)\n",
    "    X_image.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_PM25 = []\n",
    "with open(\"./y_PM25_full.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        y_PM25.append(np.float32(float(line.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train-Test Split\n",
    "\n",
    "dataset = list(zip(X_image, y_PM25))\n",
    "\n",
    "###shuffle dataset to get train, test datasets\n",
    "random.Random(42).shuffle(dataset)\n",
    "#random.shuffle(dataset)\n",
    "batch_size = 100\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = int(np.ceil(8 * total_batches / 10))\n",
    "test_batches = total_batches - train_batches\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([item[0] for item in train]).reshape(-1,image_size,image_size,3)\n",
    "X_test = np.array([item[0] for item in test]).reshape(-1,image_size,image_size,3)\n",
    "y_train = np.array([item[1] for item in train]).reshape(-1)\n",
    "y_test = np.array([item[1] for item in test]).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative fine-tuning VGG16 for an exxample 670 m resolution model (the 500 m and 200 m models are pretty much the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_finetune = VGG16(include_top=False, weights='imagenet', \\\n",
    "                            input_tensor=None, input_shape=(image_size, image_size, 3), pooling='max', classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in VGG_finetune.layers[:]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = VGG_finetune.output\n",
    "x = Dropout(0.5)(x) # 500, 200m models = 0.1\n",
    "x = Dense(128, activation=\"relu\")(x) #500=100, 200 m =80 \n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation=None)(x)\n",
    "# creating the final model \n",
    "model_final = Model(input = VGG_finetune.input, output = predictions)\n",
    "model_final.compile(optimizer=optimizers.Adam(), loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "                rotation_range=30, width_shift_range=0.1, height_shift_range=0.1, zoom_range = 0.5,\\\n",
    "                            fill_mode = 'reflect', horizontal_flip = True,vertical_flip=True, channel_shift_range = 0)\n",
    "\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "validation_generator = test_datagen.flow(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='./checkpoint_VGG16_finetune_670m/checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "#early = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_final.fit_generator(train_generator, steps_per_epoch=len(X_train)//batch_size, epochs=60, verbose=1, callbacks=[checkpoint,history], validation_data=validation_generator, validation_steps=len(y_train)//batch_size\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about retrain the last conv. layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_finetune = VGG16(include_top=False, weights='imagenet', \\\n",
    "                            input_tensor=None, input_shape=(image_size, image_size, 3), pooling='max', classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in VGG_finetune.layers[:-3]:\n",
    "    layer.trainable = False\n",
    "for layer in VGG_finetune.layers[-3:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_finetune.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your best model from the last training here\n",
    "model = load_model('') #670m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_128 = model.layers[-3].get_weights()\n",
    "\n",
    "weights_1 = model.layers[-1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = VGG_finetune.output\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation=\"relu\",weights = weights_128)(x) \n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation=None, weights = weights_1)(x)\n",
    "# creating the final model \n",
    "model_final = Model(input = VGG_finetune.input, output = predictions)\n",
    "model_final.compile(optimizer=optimizers.Adam(), loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_final.layers[-3:]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.compile(optimizer=optimizers.Adam(), loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "                rotation_range=30, width_shift_range=0.1, height_shift_range=0.1, zoom_range = 0.5,\\\n",
    "                            fill_mode = 'reflect', horizontal_flip = True,vertical_flip=True, channel_shift_range = 0)\n",
    "\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "validation_generator = test_datagen.flow(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='./checkpoint_VGG16_finetune_670m_last_conv/checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "#early = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_final.fit_generator(train_generator, steps_per_epoch=len(X_train)//batch_size, epochs=60, verbose=1, callbacks=[checkpoint,history], validation_data=validation_generator, validation_steps=len(y_train)//batch_size\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about further retraining the second to last conv. layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again load your best model from the last training here\n",
    "model = load_model('') #670m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_lastconv = model.layers[-7].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_finetune = VGG16(include_top=False, weights='imagenet', \\\n",
    "                            input_tensor=None, input_shape=(image_size, image_size, 3), pooling='max', classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in VGG_finetune.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "for layer in VGG_finetune.layers[-3:]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_finetune.layers[-3].set_weights(weights_lastconv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VGG_finetune.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_128 = model.layers[-3].get_weights()\n",
    "weights_1 = model.layers[-1].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = VGG_finetune.output\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation=\"relu\",  weights = weights_128)(x) #500m = 100\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation=None, weights = weights_1)(x)\n",
    "# creating the final model \n",
    "model_final = Model(input = VGG_finetune.input, output = predictions)\n",
    "model_final.compile(optimizer=optimizers.Adam(), loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_final.layers[-3:]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_final.compile(optimizer=optimizers.Adam(), loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "                rotation_range=30, width_shift_range=0.1, height_shift_range=0.1, zoom_range = 0.5,\\\n",
    "                            fill_mode = 'reflect', horizontal_flip = True,vertical_flip=True, channel_shift_range = 0)\n",
    "\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "validation_generator = test_datagen.flow(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='./checkpoint_VGG16_finetune_670m_seclast_conv/checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "#early = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_final.fit_generator(train_generator, steps_per_epoch=len(X_train)//batch_size, epochs=60, verbose=1, callbacks=[checkpoint,history], validation_data=validation_generator, validation_steps=len(y_train)//batch_size\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about further retraining the third to last conv. layer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again load your best model from the last training here\n",
    "model = load_model('') #670m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_lastconv = model.layers[-7].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_2tolastconv = model.layers[-8].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_128 = model.layers[-3].get_weights()\n",
    "weights_1 = model.layers[-1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_finetune = VGG16(include_top=False, weights='imagenet', \\\n",
    "                            input_tensor=None, input_shape=(image_size, image_size, 3), pooling='max', classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in VGG_finetune.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "for layer in VGG_finetune.layers[-4:]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_finetune.layers[-3].set_weights(weights_lastconv)\n",
    "VGG_finetune.layers[-4].set_weights(weights_2tolastconv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "VGG_finetune.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = VGG_finetune.output\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation=\"relu\", weights = weights_128)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation=None, weights = weights_1)(x)\n",
    "# creating the final model \n",
    "model_final = Model(input = VGG_finetune.input, output = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_final.layers[-3:]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.compile(optimizer=optimizers.Adam(), loss='mean_squared_error', metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "                rotation_range=30, width_shift_range=0.1, height_shift_range=0.1, zoom_range = 0.5,\\\n",
    "                            fill_mode = 'reflect', horizontal_flip = True,vertical_flip=True, channel_shift_range = 0)\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "validation_generator = test_datagen.flow(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='./checkpoint_VGG16_finetune_670m_thirdlast_conv/checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "#early = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_final.fit_generator(train_generator, steps_per_epoch=len(X_train)//batch_size, epochs=80, verbose=1, callbacks=[checkpoint,history], validation_data=validation_generator, validation_steps=len(y_train)//batch_size\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF part and final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the final best model here!\n",
    "\n",
    "#670m\n",
    "model = load_model('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "extract_model = Model(inputs=model.input, outputs=model.get_layer('dense_1').output) #may not be dense_1, \n",
    "# should be the dense layer with dimension 128 for 670 m model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_image_features = extract_model.predict(np.array(X_image).reshape(-1,image_size,image_size,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_image_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if I use meteorology features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_feature = np.load('meteo_feature_full.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(meteo_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_meteo = np.append(X_image_features,meteo_feature,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.any(np.isnan(features_meteo), axis=1)\n",
    "features_meteo = features_meteo[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all= np.array(y_PM25).reshape([13022,-1])[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(features_meteo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in site labels and time index to see the seasonal trend and site-specific trend\n",
    "time_stamps = np.load('time_stamp_full.npy')\n",
    "site_labels = np.load('site_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamps = np.array([float(x) for x in time_stamps ])\n",
    "site_labels = np.array([float(x) for x in site_labels ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_meteo = np.append(features_meteo,time_stamps.reshape(13022,1),axis = 1)\n",
    "features_meteo = np.append(features_meteo,site_labels.reshape(13022,1),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to match RF's train test splits with the CNN's splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_RF = list(zip(features_meteo, y_all))\n",
    "\n",
    "###shuffle dataset to get train, test datasets\n",
    "random.Random(42).shuffle(dataset_RF)\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "total_batches = len(dataset_RF) // batch_size\n",
    "train_batches = int(np.ceil(8 * total_batches / 10))\n",
    "test_batches = total_batches - train_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full, test_full = dataset_RF[:train_batches*batch_size], dataset_RF[train_batches*batch_size:]\n",
    "X_train_full = np.array([x[0] for x in train_full])\n",
    "X_test_full = np.array([x[0] for x in test_full])\n",
    "y_train = np.array([x[1] for x in train_full])\n",
    "y_test = np.array([x[1] for x in test_full])\n",
    "X_train = X_train_full[:, :-2]\n",
    "X_test = X_test_full[:, :-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle=KFold(n_splits=5, shuffle=True,random_state=42)\n",
    "max_features=['auto']\n",
    "n_estimators=['300','400','500']\n",
    "#min_samples_leaf=np.linspace(2,8,3)\n",
    "min_samples_leaf=['1','2','3']\n",
    "avg_MSE=np.zeros([len(max_features),len(n_estimators),len(min_samples_leaf)])\n",
    "avg_MAE = np.zeros([len(max_features),len(n_estimators),len(min_samples_leaf)])\n",
    "for k, feature in enumerate(max_features):\n",
    "    for i,estimators in enumerate(n_estimators):\n",
    "        for j,samples_leaf in enumerate(min_samples_leaf):\n",
    "            print('k = '+str(k)+', i = '+str(i)+', j = '+str(j)+' started.')\n",
    "            scaler=StandardScaler()\n",
    "            cls = RandomForestRegressor(bootstrap=True, n_jobs=-1, random_state = 40,max_features =feature,n_estimators =int(estimators),min_samples_leaf=int(samples_leaf ) )\n",
    "            our_pipeline=Pipeline([('scaler',scaler),('cls',cls)])\n",
    "            MSE = -cross_val_score(our_pipeline,X_train, y_train.ravel(), cv=shuffle, scoring='neg_mean_squared_error')\n",
    "            avg_MSE[k,i,j]=np.mean(MSE)\n",
    "            #MAE = -cross_val_score(our_pipeline,X_train, y_train.ravel(), cv=shuffle, scoring='neg_mean_absolute_error')\n",
    "            #avg_MAE[k,i,j]=np.mean(MAE)\n",
    "            print('k = '+str(k)+', i = '+str(i)+', j = '+str(j)+' finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(avg_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute 5-fold results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_cv_results(X,y):\n",
    "    CV5_spearman = []\n",
    "    CV5_pearson = []\n",
    "    CV5_MAE = []\n",
    "    CV5_NMAE = []\n",
    "    CV5_RMSE = []\n",
    "    CV5_NRMSE = []\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        scaler=StandardScaler()\n",
    "        cls = RandomForestRegressor(bootstrap=True, n_jobs=-1, random_state = 40,max_features ='auto',n_estimators =500,min_samples_leaf=1 )\n",
    "        our_pipeline=Pipeline([('scaler',scaler),('cls',cls)])\n",
    "        our_pipeline.fit(X_train,y_train.ravel())\n",
    "        CV5_spearman.append(stats.spearmanr((our_pipeline.predict(X_test)),y_test.ravel())[0])\n",
    "        CV5_pearson.append(stats.pearsonr((our_pipeline.predict(X_test)),y_test.ravel())[0])\n",
    "        my_prediction = our_pipeline.predict(X_test)\n",
    "        RMSE = round(np.sqrt(metrics.mean_squared_error(y_test, my_prediction)),1)\n",
    "        MAE = round(metrics.mean_absolute_error(y_test, my_prediction),1)\n",
    "        CV5_MAE.append(MAE)\n",
    "        CV5_NMAE.append(MAE/np.mean(y_test))\n",
    "        CV5_RMSE.append(RMSE)\n",
    "        CV5_NRMSE.append(RMSE/np.mean(y_test))\n",
    "\n",
    "    return  CV5_spearman, CV5_pearson, CV5_MAE, CV5_NMAE, CV5_RMSE, CV5_NRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " CV5_spearman, CV5_pearson, CV5_MAE, CV5_NMAE, CV5_RMSE, CV5_NRMSE = final_cv_results(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV5_mean = [np.mean(CV5_spearman),np.mean(CV5_pearson),np.mean(CV5_MAE), np.mean(CV5_NMAE), np.mean(CV5_RMSE), np.mean(CV5_NRMSE) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV5_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illustrating 5-fold CV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_indices(cv, X, y, ax, n_splits, lw=40):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "    for ii, value in enumerate(['Fold 0', 'Fold 1', 'Fold 2', 'Fold 3', 'Fold 4']):\n",
    "        plt.text(1000+2000*ii, 6-0.4,value,horizontalalignment='center', fontsize=20, rotation=0, color ='black' )\n",
    "    for ii, value in enumerate(CV5_spearman):\n",
    "        plt.text(1000+2000*ii, 7-0.4,round(value,2),horizontalalignment='center', fontsize=20, rotation=0, color ='black' )\n",
    "    for ii, value in enumerate(CV5_pearson):\n",
    "        plt.text(1000+2000*ii, 8-0.4,round(value,2),horizontalalignment='center', fontsize=20, rotation=0, color ='black' )\n",
    "    for ii, value in enumerate(CV5_MAE):\n",
    "        plt.text(1000+2000*ii, 9-0.4,round(value,1),horizontalalignment='center', fontsize=20, rotation=0, color ='black' )\n",
    "    for ii, value in enumerate(CV5_NMAE):\n",
    "        plt.text(1000+2000*ii, 10-0.4,str(round(value*100,1))+'%',horizontalalignment='center', fontsize=20, rotation=0, color ='black' )\n",
    "    for ii, value in enumerate(CV5_RMSE):\n",
    "        plt.text(1000+2000*ii, 11-0.4,round(value,1),horizontalalignment='center', fontsize=20, rotation=0, color ='black' )\n",
    "    for ii, value in enumerate(CV5_NRMSE):\n",
    "        plt.text(1000+2000*ii, 12-0.4,str(round(value*100,1))+'%',horizontalalignment='center', fontsize=20, rotation=0, color ='black' )\n",
    "    ''' \n",
    "    for ii, value in enumerate(CV5_mean):\n",
    "        if ii==0 or 1:\n",
    "            plt.text(1000+2000*ii, 12-0.2,round(value,2),horizontalalignment='center', fontsize=15, rotation=0, color ='black' )\n",
    "        elif ii==2 or 4:\n",
    "            plt.text(1000+2000*ii, 12-0.2,round(value,1),horizontalalignment='center', fontsize=15, rotation=0, color ='black' )\n",
    "\n",
    "        else:\n",
    "            plt.text(1000+2000*ii, 12-0.2,str(round(value*100,1))+'%',horizontalalignment='center', fontsize=15, rotation=0, color ='black' )\n",
    "    '''\n",
    "    # Plot the data classes and groups at the end\n",
    "    #ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "    #           c=y, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    #ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "    #           c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['Folds','Spearman r','Pearson r','MAE', 'NMAE','RMSE','NRMSE']\n",
    "    ax.set(yticks=np.arange(n_splits+7) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+7.2, -.2], xlim=[0, len(X)])\n",
    "    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n",
    "              ['CV test', 'CV training'], loc=(-0.5, 0.8), fontsize=20)\n",
    "    ax.set_title('5-fold CV results on the Beijing training set', fontsize=20)\n",
    "    ax.tick_params(labelsize = 20)\n",
    "    ax.set_xlabel('Sample index',size = 20)\n",
    "    ax.set_ylabel(\"CV iteration\", size = 20)\n",
    "    plt.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "cv = KFold(n_splits=5, shuffle=True,random_state=42)\n",
    "plot_cv_indices(cv, X_train, y_train, ax, n_splits)\n",
    "\n",
    "savepdf(fig,'cv_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model based on your best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#500 1 for VGG fine-tuning\n",
    "scaler=StandardScaler()\n",
    "cls = RandomForestRegressor(bootstrap=True, n_jobs=-1, random_state = 40,max_features ='auto',n_estimators =500,min_samples_leaf=1 )\n",
    "our_pipeline=Pipeline([('scaler',scaler),('cls',cls)])\n",
    "our_pipeline.fit(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsquared = stats.spearmanr((our_pipeline.predict(X_train)),y_train.ravel())[0]\n",
    "pvalue = stats.spearmanr((our_pipeline.predict(X_train)),y_train.ravel())[1]\n",
    "Rsquared_pearson = stats.pearsonr((our_pipeline.predict(X_train)),y_train.ravel())[0]\n",
    "pvalue_pearson = stats.pearsonr((our_pipeline.predict(X_train)),y_train.ravel())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'mathtext.default':  'regular' })\n",
    "my_prediction = our_pipeline.predict(X_train)\n",
    "RMSE = round(np.sqrt(metrics.mean_squared_error(y_train, my_prediction)),1)\n",
    "MAE = round(metrics.mean_absolute_error(y_train, my_prediction),1)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.scatter(y_train, my_prediction,color = 'orange', edgecolors=(0, 0, 0),  s = 100)\n",
    "ax.plot([0, 400], [0, 400], 'k--', lw=4)\n",
    "ax.set_xlabel('True $PM_{2.5}$ ($\\mu $g m$^{-3}$)', size = 20)\n",
    "ax.set_ylabel('Predicted $PM_{2.5}$ ($\\mu $g m$^{-3}$)', size = 20)\n",
    "ax.tick_params(labelsize = 20)\n",
    "fig.text(0.15, 0.83, 'Spearman r = '+ str(round(Rsquared,2)) + ' (p-value = '+ str(round(pvalue,7))+')', color='black', weight='roman',\n",
    "fontsize=20)\n",
    "\n",
    "plt.axis('tight')\n",
    "fig.text(0.15, 0.79, 'Pearson r = '+ str(round(Rsquared_pearson,2)) + ' (p-value = '+ str(round(pvalue_pearson,100)) + ')', color='black', weight='roman',\n",
    "fontsize=20)\n",
    "\n",
    "fig.text(0.15, 0.75, 'RMSE = '+ str(RMSE), color='black', weight='roman',\n",
    "fontsize=20)\n",
    "fig.text(0.15, 0.71, 'MAE = '+ str(MAE), color='black', weight='roman',\n",
    "fontsize=20)\n",
    "fig.text(0.15, 0.67, 'NRMSE = '+ str(round(RMSE/np.mean(y_train)*100,1))+'%', color='black', weight='roman',\n",
    "fontsize=20)\n",
    "fig.text(0.15, 0.63, 'NMAE = '+ str(round(MAE/np.mean(y_train)*100,1))+'%', color='black', weight='roman',\n",
    "fontsize=20)\n",
    "fig.text(0.567, 0.135, 'Beijing training dataset', bbox=dict(facecolor='grey', alpha=0.9),color='black', weight='roman',\n",
    "fontsize=20)\n",
    "\n",
    "#plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "#savepdf(fig,'Satellite_Beijing_VGGtune_training_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsquared = stats.spearmanr((our_pipeline.predict(X_test)),y_test.ravel())[0]\n",
    "pvalue = stats.spearmanr((our_pipeline.predict(X_test)),y_test.ravel())[1]\n",
    "Rsquared_pearson = stats.pearsonr((our_pipeline.predict(X_test)),y_test.ravel())[0]\n",
    "pvalue_pearson = stats.pearsonr((our_pipeline.predict(X_test)),y_test.ravel())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'mathtext.default':  'regular' })\n",
    "my_prediction = our_pipeline.predict(X_test)\n",
    "RMSE = round(np.sqrt(metrics.mean_squared_error(y_test, my_prediction)),1)\n",
    "MAE = round(metrics.mean_absolute_error(y_test, my_prediction),1)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.scatter(y_test, my_prediction, color = 'royalblue',edgecolors=(0, 0, 0), s = 100)\n",
    "ax.plot([0, 400], [0, 400], 'k--', lw=4)\n",
    "ax.set_xlabel('True $PM_{2.5}$ ($\\mu $g m$^{-3}$)', size = 30)\n",
    "ax.set_ylabel('Predicted $PM_{2.5}$ ($\\mu $g m$^{-3}$)', size = 30)\n",
    "ax.tick_params(labelsize = 30)\n",
    "fig.text(0.15, 0.85, 'Spearman r = '+ str(round(Rsquared,2)) + ' (p-value = '+ str(round(pvalue,7))+')', color='black', weight='roman',\n",
    "fontsize=25)\n",
    "\n",
    "plt.axis('tight')\n",
    "fig.text(0.15, 0.81, 'Pearson r = '+ str(round(Rsquared_pearson,2)) + ' (p-value = '+ str(round(pvalue_pearson,2)) + ')', color='black', weight='roman',\n",
    "fontsize=25)\n",
    "\n",
    "fig.text(0.15, 0.77, 'RMSE = '+ str(RMSE), color='black', weight='roman',\n",
    "fontsize=25)\n",
    "fig.text(0.15, 0.73, 'MAE = '+ str(MAE), color='black', weight='roman',\n",
    "fontsize=25)\n",
    "fig.text(0.15, 0.69, 'NRMSE = '+ str(round(RMSE/np.mean(y_test)*100,1))+'%', color='black', weight='roman',\n",
    "fontsize=25)\n",
    "fig.text(0.15, 0.65, 'NMAE = '+ str(round(MAE/np.mean(y_test)*100,1))+'%', color='black', weight='roman',\n",
    "fontsize=25)\n",
    "fig.text(0.49, 0.14, 'Beijing test dataset', bbox=dict(facecolor='grey', alpha=0.9),color='black', weight='roman',\n",
    "fontsize=30)\n",
    "#plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.gcf().subplots_adjust(left=0.15)\n",
    "plt.show()\n",
    "savepdf(fig,'Satellite_Beijing_VGGtune_test_best')\n",
    "#savepdf(fig,'Satellite_Beijing_VGG16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = cls.feature_importances_\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in cls.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "importances\n",
    "\n",
    "#indices[:10]\n",
    "\n",
    "importance_indices = []\n",
    "for i in range(128):\n",
    "    importance_indices.append('Image_'+str(i))\n",
    "importance_indices.append('Temperature')\n",
    "importance_indices.append('RH')\n",
    "importance_indices.append('Wind speed')\n",
    "importance_indices.append('SLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize = (20,20))\n",
    "\n",
    "num_feature = 10\n",
    "plt.bar(range(num_feature), importances[indices][:num_feature],\n",
    "      color=\"darkred\", align=\"center\")\n",
    "plt.xticks(range(num_feature), np.array(importance_indices)[indices][:num_feature], rotation=45,size=35)\n",
    "plt.yticks(size=40, rotation=45)\n",
    "plt.xlabel('Top'+ str(num_feature) +' features',size = 40)\n",
    "plt.ylabel('Importance',size = 40)\n",
    "fig.text(0.695, 0.862, 'Beijing test dataset', bbox=dict(facecolor='grey', alpha=0.9),color='black', weight='roman',\n",
    "fontsize=30)\n",
    "fig.subplots_adjust(bottom=0.3, left = 0.2)\n",
    "plt.show()\n",
    "#savepdf(fig,'Feature_importances VGG16tune_Beijing_top10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-bag sample (US EMBASSY) test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_X_images_folder = './X_image_embassy'\n",
    "image_files = [f for f in listdir(my_X_images_folder) if isfile(join(my_X_images_folder, f)) and '.DS_Store' not in f ]\n",
    "image_files_sorted = sorted(image_files, key = lambda x: int(x.split('.')[0].split('_')[2]))\n",
    "\n",
    "image_files_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_image_embassy = []\n",
    "image_size = 224\n",
    "for image_file in image_files_sorted:\n",
    "    img = image.load_img(my_X_images_folder+'/'+image_file,target_size=(image_size , image_size ))\n",
    "    x = image.img_to_array(img) #(224,224,3)\n",
    "    x = np.expand_dims(x, axis=0) #(1,224,224,3)\n",
    "    x = preprocess_input(x)\n",
    "    X_image_embassy.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_PM25_embassy = []\n",
    "with open(\"./y_PM25_embassy.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        y_PM25_embassy.append(np.float32(float(line.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_image_embassy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_image_embassy_features = extract_model.predict(np.array(X_image_embassy).reshape(-1,image_size ,image_size ,3))\n",
    "\n",
    "X_image_embassy_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_feature_embassy = np.load('meteo_feature_embassy.npy')\n",
    "features_meteo_embassy = np.append(X_image_embassy_features,meteo_feature_embassy,axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "mask = ~np.any(np.isnan(features_meteo_embassy), axis=1)\n",
    "features_meteo_embassy = features_meteo_embassy[mask]\n",
    "\n",
    "y_all_embassy= np.array(y_PM25_embassy).reshape([373,-1])[mask]\n",
    "\n",
    "features_meteo_embassy\n",
    "\n",
    "y_all_embassy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_embassy = our_pipeline.predict(features_meteo_embassy)\n",
    "\n",
    "np.sqrt(round(metrics.mean_squared_error(y_all_embassy, prediction_embassy),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsquared = stats.spearmanr(prediction_embassy,y_all_embassy.ravel())[0]\n",
    "pvalue = stats.spearmanr(prediction_embassy,y_all_embassy.ravel())[1]\n",
    "Rsquared_pearson = stats.pearsonr(prediction_embassy,y_all_embassy.ravel())[0]\n",
    "pvalue_pearson = stats.pearsonr(prediction_embassy,y_all_embassy.ravel())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = round(np.sqrt(metrics.mean_squared_error(y_all_embassy, prediction_embassy)),1)\n",
    "MAE = round(metrics.mean_absolute_error(y_all_embassy, prediction_embassy),1)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.scatter(y_all_embassy, prediction_embassy, color = 'plum',edgecolors=(0, 0, 0), s = 100)\n",
    "ax.plot([0, 400], [0, 400], 'k--', lw=4)\n",
    "ax.set_xlabel('True $PM_{2.5}$ ($\\mu $g m$^{-3}$)', size = 30)\n",
    "ax.set_ylabel('Predicted $PM_{2.5}$ ($\\mu $g m$^{-3}$)', size = 30)\n",
    "ax.tick_params(labelsize = 30)\n",
    "fig.text(0.15, 0.85, 'Spearman r = '+ str(round(Rsquared,2)) + ' (p-value = '+ str(round(pvalue,7))+')', color='black', weight='roman',\n",
    "fontsize=25)\n",
    "\n",
    "plt.axis('tight')\n",
    "fig.text(0.15, 0.81, 'Pearson r = '+ str(round(Rsquared_pearson,2)) + ' (p-value = '+ str(round(pvalue_pearson,2)) + ')', color='black', weight='roman',\n",
    "fontsize=25)\n",
    "\n",
    "fig.text(0.15, 0.77, 'RMSE = '+ str(RMSE), color='black', weight='roman',\n",
    "fontsize=25)\n",
    "fig.text(0.15, 0.73, 'MAE = '+ str(MAE), color='black', weight='roman',\n",
    "fontsize=25)\n",
    "fig.text(0.15, 0.69, 'NRMSE = '+ str(round(RMSE/np.mean(y_all_embassy)*100,1))+'%', color='black', weight='roman',\n",
    "fontsize=25)\n",
    "fig.text(0.15, 0.65, 'NMAE = '+ str(round(MAE/np.mean(y_all_embassy)*100,1))+'%', color='black', weight='roman',\n",
    "fontsize=25)\n",
    "fig.text(0.305, 0.14, 'Embassy out-of-bag dataset', bbox=dict(facecolor='grey', alpha=0.9),color='black', weight='roman',\n",
    "fontsize=30)\n",
    "plt.gcf().subplots_adjust(left=0.15)\n",
    "plt.show()\n",
    "savepdf(fig,'Embassy_out_of_bag_VGG16fine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally Shanghai Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_X_images_folder = './X_image_Shanghai'\n",
    "image_files = [f for f in listdir(my_X_images_folder) if isfile(join(my_X_images_folder, f)) and '.DS_Store' not in f ]\n",
    "image_files_sorted = sorted(image_files, key = lambda x: int(x.split('.')[0].split('_')[2]))\n",
    "\n",
    "image_files_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_image_shanghai = []\n",
    "for image_file in image_files_sorted:\n",
    "    img = image.load_img(my_X_images_folder+'/'+image_file,target_size=(image_size , image_size ))\n",
    "    x = image.img_to_array(img) #(224,224,3)\n",
    "    x = np.expand_dims(x, axis=0) #(1,224,224,3)\n",
    "    x = preprocess_input(x)\n",
    "    X_image_shanghai.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_PM25_shanghai = []\n",
    "with open(\"./y_PM25_Shanghai.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        y_PM25_shanghai.append(np.float32(float(line.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_image_shanghai_features = extract_model.predict(np.array(X_image_shanghai).reshape(-1,image_size ,image_size ,3))\n",
    "\n",
    "X_image_shanghai_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_feature_shanghai = np.load('meteo_feature_shanghai.npy')\n",
    "features_meteo_shanghai = np.append(X_image_shanghai_features,meteo_feature_shanghai,axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "mask = ~np.any(np.isnan(features_meteo_shanghai), axis=1)\n",
    "features_meteo_shanghai = features_meteo_shanghai[mask]\n",
    "\n",
    "y_all_shanghai= np.array(y_PM25_shanghai).reshape([1897,-1])[mask]\n",
    "\n",
    "prediction_shanghai = our_pipeline.predict(features_meteo_shanghai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsquared = stats.spearmanr(prediction_shanghai ,y_all_shanghai .ravel())[0]\n",
    "pvalue = stats.spearmanr(prediction_shanghai ,y_all_shanghai .ravel())[1]\n",
    "Rsquared_pearson = stats.pearsonr(prediction_shanghai ,y_all_shanghai .ravel())[0]\n",
    "pvalue_pearson = stats.pearsonr(prediction_shanghai ,y_all_shanghai .ravel())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = round(np.sqrt(metrics.mean_squared_error(y_all_shanghai , prediction_shanghai)),1)\n",
    "MAE = round(metrics.mean_absolute_error(y_all_shanghai , prediction_shanghai),1)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.scatter(y_all_shanghai , prediction_shanghai, color = 'gold',edgecolors=(0, 0, 0), s = 100)\n",
    "ax.plot([0, 400], [0, 400], 'k--', lw=4)\n",
    "ax.set_xlabel('True $PM_{2.5}$ ($\\mu $g m$^{-3}$)', size = 30)\n",
    "ax.set_ylabel('Predicted $PM_{2.5}$ ($\\mu $g m$^{-3}$)', size = 30)\n",
    "ax.tick_params(labelsize = 30)\n",
    "fig.text(0.15, 0.85, 'Spearman r = '+ str(round(Rsquared,2)) + ' (p-value = '+ str(round(pvalue,7))+')', color='black', weight='roman',\n",
    "fontsize=25)\n",
    "\n",
    "plt.axis('tight')\n",
    "fig.text(0.15, 0.81, 'Pearson r = '+ str(round(Rsquared_pearson,2)) + ' (p-value = '+ str(round(pvalue_pearson,2)) + ')', color='black', weight='roman',\n",
    "fontsize=25)\n",
    "\n",
    "fig.text(0.15, 0.77, 'RMSE = '+ str(RMSE), color='black', weight='roman',\n",
    "fontsize=25)\n",
    "fig.text(0.15, 0.73, 'MAE = '+ str(MAE), color='black', weight='roman',\n",
    "fontsize=25)\n",
    "fig.text(0.15, 0.69, 'NRMSE = '+ str(round(RMSE/np.mean(y_all_shanghai)*100,1))+'%', color='black', weight='roman',\n",
    "fontsize=25)\n",
    "fig.text(0.15, 0.65, 'NMAE = '+ str(round(MAE/np.mean(y_all_shanghai)*100,1))+'%', color='black', weight='roman',\n",
    "fontsize=25)\n",
    "fig.text(0.258, 0.14, 'Shanghai dataset (no training)', bbox=dict(facecolor='grey', alpha=0.9),color='black', weight='roman',\n",
    "fontsize=30)\n",
    "plt.gcf().subplots_adjust(left=0.15)\n",
    "plt.show()\n",
    "savepdf(fig,'Shanghai_VGG16fine_no_training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For shanghai, only re-train random forest then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_feature_shanghai = np.load('meteo_feature_shanghai.npy')\n",
    "features_meteo_shanghai = np.append(X_image_shanghai_features,meteo_feature_shanghai,axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "mask = ~np.any(np.isnan(features_meteo_shanghai), axis=1)\n",
    "features_meteo_shanghai = features_meteo_shanghai[mask]\n",
    "\n",
    "y_all_shanghai= np.array(y_PM25_shanghai).reshape([1897,-1])[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in site labels and time index to see the seasonal trend and site-specific trend\n",
    "time_stamps = np.load('time_stamp_shanghai.npy')\n",
    "site_labels = np.load('site_label_shanghai.npy')\n",
    "\n",
    "time_stamps = np.array([float(x) for x in time_stamps ])\n",
    "site_labels = np.array([float(x) for x in site_labels ])\n",
    "\n",
    "features_meteo = np.append(features_meteo_shanghai,time_stamps.reshape(1897,1),axis = 1)\n",
    "features_meteo = np.append(features_meteo_shanghai,site_labels.reshape(1897,1),axis = 1)\n",
    "\n",
    "X_train_full, X_test_full, y_train, y_test = train_test_split(features_meteo , y_all_shanghai, test_size=0.2, random_state=0)\n",
    "X_train = X_train_full[:, :-2]\n",
    "X_test = X_test_full[:, :-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "cls = RandomForestRegressor(bootstrap=True, n_jobs=-1, random_state = 40,max_features ='auto',n_estimators =500,min_samples_leaf=1 )\n",
    "our_pipeline=Pipeline([('scaler',scaler),('cls',cls)])\n",
    "our_pipeline.fit(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsquared = stats.spearmanr((our_pipeline.predict(X_test)),y_test.ravel())[0]\n",
    "pvalue = stats.spearmanr((our_pipeline.predict(X_test)),y_test.ravel())[1]\n",
    "Rsquared_pearson = stats.pearsonr((our_pipeline.predict(X_test)),y_test.ravel())[0]\n",
    "pvalue_pearson = stats.pearsonr((our_pipeline.predict(X_test)),y_test.ravel())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prediction = our_pipeline.predict(X_test)\n",
    "RMSE = round(np.sqrt(metrics.mean_squared_error(y_test, my_prediction)),1)\n",
    "MAE = round(metrics.mean_absolute_error(y_test, my_prediction),1)\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.scatter(y_test, my_prediction, color = 'coral',edgecolors=(0, 0, 0), s = 100)\n",
    "ax.plot([0, 200], [0, 200], 'k--', lw=4)\n",
    "ax.set_xlabel('True $PM_{2.5}$ ($\\mu $g m$^{-3}$)', size = 30)\n",
    "ax.set_ylabel('Predicted $PM_{2.5}$ ($\\mu $g m$^{-3}$)', size = 30)\n",
    "ax.tick_params(labelsize = 30)\n",
    "fig.text(0.15, 0.85, 'Spearman r = '+ str(round(Rsquared,2)) + ' (p-value = '+ str(round(pvalue,7))+')', color='black', weight='roman',\n",
    "fontsize=25)\n",
    "\n",
    "plt.axis('tight')\n",
    "fig.text(0.15, 0.81, 'Pearson r = '+ str(round(Rsquared_pearson,2)) + ' (p-value = '+ str(round(pvalue_pearson,2)) + ')', color='black', weight='roman',\n",
    "fontsize=25)\n",
    "\n",
    "fig.text(0.15, 0.77, 'RMSE = '+ str(RMSE), color='black', weight='roman',\n",
    "fontsize=25)\n",
    "fig.text(0.15, 0.73, 'MAE = '+ str(MAE), color='black', weight='roman',\n",
    "fontsize=25)\n",
    "fig.text(0.15, 0.69, 'NRMSE = '+ str(round(RMSE/np.mean(y_test)*100,1))+'%', color='black', weight='roman',\n",
    "fontsize=25)\n",
    "fig.text(0.15, 0.65, 'NMAE = '+ str(round(MAE/np.mean(y_test)*100,1))+'%', color='black', weight='roman',\n",
    "fontsize=25)\n",
    "fig.text(0.21, 0.137, 'Shanghai dataset (RF re-trained)', bbox=dict(facecolor='grey', alpha=0.9),color='black', weight='roman',\n",
    "fontsize=30)\n",
    "plt.gcf().subplots_adjust(left=0.15)\n",
    "#plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "savepdf(fig,'Shanghai_VGGtune_RFretrain_best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
