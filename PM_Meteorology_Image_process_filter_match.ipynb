{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: Tongshu Zheng from Duke University\n",
    "#Email: tongshu.zheng@duke.edu; contact me if you have any questions regarding the code\n",
    "#Please reference the code source and publication (i.e., \"Estimating ground-level PM2.5 using micro-satellite \n",
    "#images by a convolutional neural network and random forest approach\") if you use the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "PROJECT_SAVE_DIR = \"Project\"\n",
    "\n",
    "import os\n",
    "if not (os.path.isdir(PROJECT_ROOT_DIR+'/'+PROJECT_SAVE_DIR)):\n",
    "    print('Figure directory didn''t exist, creating now.')\n",
    "    os.mkdir(PROJECT_ROOT_DIR+'/'+PROJECT_SAVE_DIR)\n",
    "else:\n",
    "    print('Figure directory exists.') \n",
    "    \n",
    "def savepdf(fig,name):\n",
    "    fig.savefig(PROJECT_ROOT_DIR+'/'+PROJECT_SAVE_DIR+'/'+name+'.pdf')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from scipy.interpolate import interp1d\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import json\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import time\n",
    "import shlex, subprocess\n",
    "import urllib.request\n",
    "import webbrowser\n",
    "import copy\n",
    "import imageio\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.plot import show\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first step is just to extract our PM2.5 data from the downloaded PM files for the 35 stations in Beijing over a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beijing_PM25_retriever(mypath):\n",
    "    onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f)) and 'extra' not in str(f)]\n",
    "    Beijing_Ref_names = onlyfiles\n",
    "    Beijing_Ref = []\n",
    "    for i in range(0,len(onlyfiles)):\n",
    "        try:\n",
    "            df = pd.read_csv(mypath+'/'+Beijing_Ref_names[i])\n",
    "            #df = df.loc[:,['date', 'hour', 'type', '东四', '天坛', '官园', '万寿西宫', '奥体中心', '农展馆', '万柳','前门','西直门北','东四环']]\n",
    "            df = df[df['type']=='PM2.5']\n",
    "            df['Date'] = [str(x) +  str(y) if int(str(y))>9 else str(x) + '0' + str(y) for x,y in zip(df.iloc[:,0],df.iloc[:,1])]\n",
    "            df['Date'] = df.Date.map(lambda x: datetime.strptime(x,'%Y%m%d%H'))\n",
    "            df = df.set_index(pd.DatetimeIndex(df['Date']))\n",
    "            df.drop(['Date','date','hour','type'],axis = 1,inplace=True)\n",
    "            Beijing_Ref.append(df)\n",
    "        except:\n",
    "            print(str(Beijing_Ref_names[i])+' cannot be loaded')\n",
    "    print(len(Beijing_Ref))\n",
    "    df_merge = Beijing_Ref[0]\n",
    "    for i in range(1,len(Beijing_Ref)):\n",
    "        df_merge = pd.concat([df_merge,Beijing_Ref[i]])\n",
    "    df_merge_cleaned = df_merge.resample('1h').mean()\n",
    "    return df_merge_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath_1 = './beijing_20170101-20171231 2'\n",
    "mypath_2 = './beijing_20180101-20181231'\n",
    "mypath_3 = './beijing_20190101-20190720'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_cleaned_2017 = beijing_PM25_retriever(mypath_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_cleaned_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_cleaned_2018 = beijing_PM25_retriever(mypath_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_cleaned_2019 = beijing_PM25_retriever(mypath_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_cleaned = pd.concat([df_merge_cleaned_2017,df_merge_cleaned_2018 ])\n",
    "df_merge_cleaned = pd.concat([df_merge_cleaned,df_merge_cleaned_2019])\n",
    "\n",
    "#df_merge_cleaned = df_merge_cleaned[(df_merge_cleaned.index.get_level_values(0) >= '2017-10-01') & (df_merge_cleaned.index.get_level_values(0) < '2018-10-01') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_cleaned.to_excel('PM25_2017_2019.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These codes are to sperately store each station's measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_stations_concat = []\n",
    "for i in range(len(df_merge_cleaned.columns)):\n",
    "    my_q = df_merge_cleaned[df_merge_cleaned.columns[i]].to_frame()\n",
    "    single_station = my_q.copy(deep = True)\n",
    "    single_station.dropna(inplace = True)\n",
    "    single_station = single_station.loc[~single_station.isin([np.nan, np.inf, -np.inf]).any(1),:]\n",
    "    single_station['data_counter'] = np.repeat(1, len(single_station))\n",
    "    single_station_sum = single_station.resample('1d').sum()\n",
    "    single_station_sum = single_station_sum[single_station_sum['data_counter']>=18]\n",
    "    selection = single_station_sum.index.get_level_values(0)\n",
    "    my_q = my_q.resample('1d').mean()\n",
    "    my_q = my_q.loc[selection]\n",
    "    single_stations_concat.append(my_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# if you want to process embassy PM data as well, but embassy should be constructed as a separate out-of-bag dataset (separate from the other 35 Beijng stations)\n",
    "single_stations_concat = []\n",
    "df1 = pd.read_csv('./Beijing_PM2.5_2017_YTD.csv',usecols=[2,10,13])\n",
    "df2 = pd.read_csv('./Beijing_PM2.5_2018_YTD.csv',usecols=[2,10,13])\n",
    "df3 = pd.read_csv('./Beijing_PM2.5_2019_YTD.csv',usecols=[2,10,13])\n",
    "df = pd.concat([df1,df2,df3])\n",
    "df['Date'] = pd.to_datetime(df.iloc[:,0])\n",
    "df_copy = df.copy(deep=True)\n",
    "df_copy = df_copy.set_index(pd.DatetimeIndex(df_copy['Date']))\n",
    "df_copy = df_copy.drop(columns=['Date (LT)','Date'])\n",
    "df_copy = df_copy[df_copy['QC Name']=='Valid']\n",
    "df_copy = df_copy[df_copy['Raw Conc.']>0]\n",
    "df_copy['data_counter'] = np.repeat(1, len(df_copy))\n",
    "single_station_sum = df_copy.resample('1d').sum()\n",
    "single_station_sum = single_station_sum[single_station_sum['data_counter']>=18]\n",
    "selection = single_station_sum.index.get_level_values(0)\n",
    "my_q = df_copy.resample('1d').mean()\n",
    "my_q = my_q.loc[selection]\n",
    "my_q.drop(['data_counter'],axis = 1,inplace=True)\n",
    "my_q.columns = ['US Embassy']\n",
    "single_stations_concat.append(my_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This shows the data completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([np.array(x).mean() for x in single_stations_concat ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([len(x)/931 for x in single_stations_concat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Meteorology data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_meteo_combined = pd.read_csv('meteo_Beijing_NanYuan.csv')\n",
    "my_meteo_combined = my_meteo_combined.loc[:,['Date','Temperature','Humidity','Wind Speed','Pressure']]\n",
    "my_meteo_combined = my_meteo_combined.set_index(pd.DatetimeIndex(my_meteo_combined['Date']))\n",
    "my_meteo_combined.drop(columns = 'Date',inplace = True)\n",
    "my_temperature = [(float(x.replace(' F',''))-32)*5/9 for x in my_meteo_combined['Temperature']]\n",
    "my_humidity = [float(x.replace('%','')) for x in my_meteo_combined['Humidity']]\n",
    "my_speed = [float(x.replace(' mph','')) for x in my_meteo_combined['Wind Speed']]\n",
    "my_pressure = [float(x.replace(' in',''))  if float(x.replace(' in','')) != 0 else np.nan for x in my_meteo_combined['Pressure']]\n",
    "#my_meteo_combined = my_meteo_combined.set_index(pd.DatetimeIndex(my_meteo_combined['Time']))\n",
    "#my_meteo_combined.drop(columns = 'Time',inplace = True)\n",
    "my_meteo_combined['Temperature'] = my_temperature\n",
    "my_meteo_combined['Humidity'] = my_humidity\n",
    "my_meteo_combined['Wind Speed'] = my_speed\n",
    "my_meteo_combined['Pressure'] = my_pressure\n",
    "my_meteo_combined = my_meteo_combined.resample('1d').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_meteo_combined.to_excel('meteo_Beijing_NanYuan_processed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  These codes were used to filter out the valid images such as with the blank space in image should be less than 10%; Then I stored the filter images, the matching PM25 data, matching meteorology data, and matching time_stamps (if you would like you can also do a similar thing and store the location tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beijing_locations = ['东四', '天坛', '官园', '万寿西宫', '奥体中心', '农展馆', '万柳', '北部新区', '植物园', '丰台花园',\n",
    "       '云岗', '古城', '房山', '大兴', '亦庄', '通州', '顺义', '昌平', '门头沟', '平谷', '怀柔', '密云',\n",
    "       '延庆', '定陵', '八达岭', '密云水库', '东高村', '永乐店', '榆垡', '琉璃河', '前门', '永定门内',\n",
    "       '西直门北', '南三环', '东四环']\n",
    "PROJECT_ROOT_DIR = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lat = [39.929,\n",
    "39.886,\n",
    "39.929,\n",
    "39.878,\n",
    "39.982,\n",
    "39.937,\n",
    "39.987,\n",
    "40.09,\n",
    "40.002,\n",
    "39.863,\n",
    "39.824,\n",
    "39.914,\n",
    "39.742,\n",
    "39.718,\n",
    "39.795,\n",
    "39.886,\n",
    "40.127,\n",
    "40.217,\n",
    "39.937,\n",
    "40.143,\n",
    "40.328,\n",
    "40.37,\n",
    "40.453,\n",
    "40.292,\n",
    "40.365,\n",
    "40.499,\n",
    "40.1,\n",
    "39.712,\n",
    "39.52,\n",
    "39.58,\n",
    "39.899,\n",
    "39.876,\n",
    "39.954,\n",
    "39.856,\n",
    "39.939 ]\n",
    "my_long = [116.417,\n",
    "116.407,\n",
    "116.339,\n",
    "116.352,\n",
    "116.397,\n",
    "116.461,\n",
    "116.287,\n",
    "116.174,\n",
    "116.207,\n",
    "116.279,\n",
    "116.146,\n",
    "116.184,\n",
    "116.136,\n",
    "116.404,\n",
    "116.506,\n",
    "116.663,\n",
    "116.655,\n",
    "116.23,\n",
    "116.106,\n",
    "117.1,\n",
    "116.628,\n",
    "116.832,\n",
    "115.972,\n",
    "116.22,\n",
    "115.988,\n",
    "116.911,\n",
    "117.12,\n",
    "116.783,\n",
    "116.3,\n",
    "116,\n",
    "116.395,\n",
    "116.394,\n",
    "116.349,\n",
    "116.368,\n",
    "116.483]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##deal with US EMBASSY DATA\n",
    "beijing_locations = ['US Embassy']\n",
    "my_lat = [39.955079]\n",
    "my_long = [116.467977]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_hole(image):\n",
    "    finding_hole = copy.deepcopy(image)\n",
    "    finding_hole[finding_hole==0]=1\n",
    "    finding_hole[finding_hole==255]=0\n",
    "    return np.mean(finding_hole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store images etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_image = []\n",
    "y_PM25 = []\n",
    "X_image_location_all = []\n",
    "y_PM25_location_all = []\n",
    "site_label = []\n",
    "time_stamp = []\n",
    "meteo_feature = []\n",
    "lat_station = []\n",
    "long_station = []\n",
    "blank_space = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(image_root_directory, image_save_directory_ending):\n",
    "    for i in range(len(beijing_locations)):\n",
    "        if i >= 0:\n",
    "            X_image_location = []\n",
    "            y_PM25_location = []\n",
    "            PROJECT_SAVE_DIR = beijing_locations[i]+image_save_directory_ending\n",
    "            my_current_folder_path = image_root_directory+'/'+PROJECT_SAVE_DIR\n",
    "            all_image_folders = [f for f in listdir(my_current_folder_path) if '.DS_Store' not in str(f)]\n",
    "            for image_folder in all_image_folders:\n",
    "                my_current_image_path = image_root_directory+'/'+PROJECT_SAVE_DIR+'/'+image_folder\n",
    "                image_files = [f for f in listdir(my_current_image_path) if isfile(join(my_current_image_path, f)) and 'xml' not in str(f)]\n",
    "                final_image_path = my_current_image_path + '/'+image_files[0]\n",
    "                #print(image_files)\n",
    "                try:\n",
    "                    im = imageio.imread(final_image_path)\n",
    "                except:\n",
    "                    print(final_image_path+' cannot be loaded')\n",
    "                    continue\n",
    "                hole = finding_hole(im[:,:,3])\n",
    "                if hole > 0.10:\n",
    "                    continue\n",
    "                else:\n",
    "                    #im = im[:,:,0:3]\n",
    "                    image_time_index = image_files[0].split('_')[0]\n",
    "                    try:\n",
    "                        matching_PM25 = single_stations_concat[i][(single_stations_concat[i].index.get_level_values(0) == image_time_index)].iloc[0,0]\n",
    "                    except: \n",
    "                        continue\n",
    "                    try:\n",
    "                        matching_meteo = list(my_meteo_combined[my_meteo_combined.index.get_level_values(0) == image_time_index].iloc[0,:])\n",
    "                    except:\n",
    "                        continue\n",
    "                    try:\n",
    "                        X_image.append(im)\n",
    "                        X_image_location.append(im)\n",
    "                        y_PM25.append(matching_PM25)\n",
    "                        y_PM25_location.append(matching_PM25)\n",
    "                        meteo_feature.append(matching_meteo)\n",
    "                        time_stamp.append(image_time_index)\n",
    "                        lat_station.append(my_lat[i])\n",
    "                        long_station.append(my_long[i])\n",
    "                        blank_space.append(hole)\n",
    "                        site_label.append(i)\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            X_image_location_all.append(X_image_location)\n",
    "            y_PM25_location_all.append(y_PM25_location)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_loader('./Beijing_2017','_2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_loader('.','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_loader('./Beijing_2019','_2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(35):\n",
    "    print(np.array([x==i for x in site_label]).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now handle Shanghai Data (pretty much the same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def PM25_retriever(mypath, site_codes, site_names):\n",
    "    onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f)) and 'extra' not in str(f)]\n",
    "    #Beijing_Ref_names = onlyfiles\n",
    "    Ref = []\n",
    "    for i in range(0,len(onlyfiles)):\n",
    "        try:\n",
    "            df = pd.read_csv(mypath+'/'+onlyfiles[i])\n",
    "            #df = df.loc[:,['date', 'hour', 'type', '东四', '天坛', '官园', '万寿西宫', '奥体中心', '农展馆', '万柳','前门','西直门北','东四环']]\n",
    "            df = df[df['type']=='PM2.5']\n",
    "            df = df.loc[:, ['date', 'hour'] + site_codes]\n",
    "            df['Date'] = [str(x) +  str(y) if int(str(y))>9 else str(x) + '0' + str(y) for x,y in zip(df.iloc[:,0],df.iloc[:,1])]\n",
    "            df['Date'] = df.Date.map(lambda x: datetime.strptime(x,'%Y%m%d%H'))\n",
    "            df = df.set_index(pd.DatetimeIndex(df['Date']))\n",
    "            df.drop(['Date','date','hour'],axis = 1,inplace=True)\n",
    "            Ref.append(df)\n",
    "        except:\n",
    "            print(str(onlyfiles[i])+' cannot be loaded')\n",
    "    print(len(Ref))\n",
    "    df_merge = Ref[0]\n",
    "    for i in range(1,len(Ref)):\n",
    "        df_merge = pd.concat([df_merge,Ref[i]])\n",
    "    df_merge_cleaned = df_merge.resample('1h').mean()\n",
    "    df_merge_cleaned.columns = site_names\n",
    "    return df_merge_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mypath_1 =  './站点_20170101-20171231'\n",
    "mypath_2 = './站点_20180101-20181231'\n",
    "mypath_3 = './站点_20190101-20190720'\n",
    "site_codes =  ['1141A','1142A','1143A','1144A','1145A','1146A','1147A','1148A','1149A','1150A']\n",
    "site_names = ['普陀', '十五厂', '虹口', '徐汇上师大', '杨浦四漂', '青浦淀山湖', '静安监测站', '浦东川沙', '浦东新区监测站', '浦东张江' ]\n",
    "\n",
    "Shanghai_2017 = PM25_retriever(mypath_1,site_codes,site_names)\n",
    "\n",
    "Shanghai_2018 = PM25_retriever(mypath_2,site_codes,site_names)\n",
    "Shanghai_2019 = PM25_retriever(mypath_3,site_codes,site_names)\n",
    "\n",
    "df_merge_cleaned = pd.concat([Shanghai_2017,Shanghai_2018,Shanghai_2019])\n",
    "#df_merge_cleaned = df_merge_cleaned[(df_merge_cleaned.index.get_level_values(0) >= '2017-10-01') & (df_merge_cleaned.index.get_level_values(0) < '2018-10-01') ]\n",
    "\n",
    "#df_merge_cleaned.to_excel('Shanghai_hourly.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single_stations_concat = []\n",
    "for i in range(len(df_merge_cleaned.columns)):\n",
    "    my_q = df_merge_cleaned[df_merge_cleaned.columns[i]].to_frame()\n",
    "    single_station = my_q.copy(deep = True)\n",
    "    single_station = single_station.loc[~single_station.isin([np.nan, np.inf, -np.inf]).any(1),:]\n",
    "    single_station['data_counter'] = np.repeat(1, len(single_station))\n",
    "    single_station_sum = single_station.resample('1d').sum()\n",
    "    single_station_sum = single_station_sum[single_station_sum['data_counter']>=18]\n",
    "    selection = single_station_sum.index.get_level_values(0)\n",
    "    my_q = my_q.resample('1d').mean()\n",
    "    my_q = my_q.loc[selection]\n",
    "    single_stations_concat.append(my_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.std([y[0]  for x in single_stations_concat for y in x.values])\n",
    "\n",
    "np.array([np.array(x).mean() for x in single_stations_concat ])\n",
    "\n",
    "np.array([len(x)/931 for x in single_stations_concat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my_meteo_combined = pd.read_csv('meteo_shanghai.csv')\n",
    "\n",
    "my_meteo_combined = my_meteo_combined.loc[:,['Date','Temperature','Humidity','Wind Speed','Pressure']]\n",
    "my_meteo_combined = my_meteo_combined.set_index(pd.DatetimeIndex(my_meteo_combined['Date']))\n",
    "my_meteo_combined.drop(columns = 'Date',inplace = True)\n",
    "my_temperature = [(float(x)-32)*5/9 for x in my_meteo_combined['Temperature']]\n",
    "my_humidity = [float(x) for x in my_meteo_combined['Humidity']]\n",
    "my_speed = [float(x) for x in my_meteo_combined['Wind Speed']]\n",
    "my_pressure = [float(x)  if float(x) != 0 else np.nan for x in my_meteo_combined['Pressure']]\n",
    "#my_meteo_combined = my_meteo_combined.set_index(pd.DatetimeIndex(my_meteo_combined['Time']))\n",
    "#my_meteo_combined.drop(columns = 'Time',inplace = True)\n",
    "my_meteo_combined['Temperature'] = my_temperature\n",
    "my_meteo_combined['Humidity'] = my_humidity\n",
    "my_meteo_combined['Wind Speed'] = my_speed\n",
    "my_meteo_combined['Pressure'] = my_pressure\n",
    "my_meteo_combined = my_meteo_combined.resample('1d').mean()\n",
    "\n",
    "my_meteo_combined\n",
    "\n",
    "my_meteo_combined.to_excel('meteo_Shanghai_hongqiao.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_image = []\n",
    "y_PM25 = []\n",
    "X_image_location_all = []\n",
    "y_PM25_location_all = []\n",
    "site_label = []\n",
    "time_stamp = []\n",
    "meteo_feature = []\n",
    "lat_station = []\n",
    "long_station = []\n",
    "blank_space = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def image_loader_shanghai(image_root_directory, image_save_directory_ending):\n",
    "    for i in range(len(site_names)):\n",
    "        if i >= 0:\n",
    "            X_image_location = []\n",
    "            y_PM25_location = []\n",
    "            PROJECT_SAVE_DIR = site_names[i]+image_save_directory_ending\n",
    "            my_current_folder_path = image_root_directory+'/'+PROJECT_SAVE_DIR\n",
    "            print(my_current_folder_path)\n",
    "            all_image_folders = [f for f in listdir(my_current_folder_path) if '.DS_Store' not in str(f)]\n",
    "            for image_folder in all_image_folders:\n",
    "                my_current_image_path = image_root_directory+'/'+PROJECT_SAVE_DIR+'/'+image_folder\n",
    "                image_files = [f for f in listdir(my_current_image_path) if isfile(join(my_current_image_path, f)) and 'xml' not in str(f)]\n",
    "                final_image_path = my_current_image_path + '/'+image_files[0]\n",
    "                #print(image_files)\n",
    "                try:\n",
    "                    im = imageio.imread(final_image_path)\n",
    "                except:\n",
    "                    print(final_image_path+' cannot be loaded')\n",
    "                    continue\n",
    "                hole = finding_hole(im[:,:,3])\n",
    "                if hole > 0.10:\n",
    "                    continue\n",
    "                else:\n",
    "                    #im = im[:,:,0:3]\n",
    "                    image_time_index = image_files[0].split('_')[0]\n",
    "                    try:\n",
    "                        matching_PM25 = single_stations_concat[i][(single_stations_concat[i].index.get_level_values(0) == image_time_index)].iloc[0,0]\n",
    "                    except: \n",
    "                        continue\n",
    "                    try:\n",
    "                        matching_meteo = list(my_meteo_combined[my_meteo_combined.index.get_level_values(0) == image_time_index].iloc[0,:])\n",
    "                    except:\n",
    "                        continue\n",
    "                    try:\n",
    "                        X_image.append(im)\n",
    "                        X_image_location.append(im)\n",
    "                        y_PM25.append(matching_PM25)\n",
    "                        y_PM25_location.append(matching_PM25)\n",
    "                        meteo_feature.append(matching_meteo)\n",
    "                        time_stamp.append(image_time_index)\n",
    "                        lat_station.append(my_lat[i])\n",
    "                        long_station.append(my_long[i])\n",
    "                        blank_space.append(hole)\n",
    "                        site_label.append(i)\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            X_image_location_all.append(X_image_location)\n",
    "            y_PM25_location_all.append(y_PM25_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image_loader_shanghai('./Shanghai','')\n",
    "\n",
    "len(X_image), len(time_stamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Temperature = [x[0] for x in meteo_feature]\n",
    "Humidity = [x[1] for x in meteo_feature]\n",
    "Wind = [x[2] for x in meteo_feature]\n",
    "SLP = [x[3] for x in meteo_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_info = {'Date':time_stamp,'Temp':Temperature, 'RH':Humidity, 'Wind':Wind,  'SLP':SLP, \\\n",
    "                'Lat':lat_station, 'Long':long_station, 'Blank_percent':blank_space, 'Site_label':site_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(SLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_info = pd.DataFrame(relevant_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_info.to_excel('other_matching_info_Beijing_full_updated.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(meteo_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_PM25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store filtered images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in enumerate(X_image):\n",
    "    if i>=0:\n",
    "        file_name = \"./X_image_Beijing/X_image_\"+str(i)+'_'+time_stamp[i]+'_'+str(site_label[i])+'.png'\n",
    "        plt.imsave(arr = x,fname = file_name)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store matching met data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('meteo_feature_Beijing.npy',meteo_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('time_stamp_Beijing.npy',time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('blank_full_Beijing.npy',blank_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('site_label_Beijing.npy',site_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store matching PM25 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./y_PM25_Beijing/y_PM25_Beijing.txt\", \"w\") as f:\n",
    "    for s in y_PM25:\n",
    "        f.write(str(s) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_PM25[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
